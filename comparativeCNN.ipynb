{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 11:41:41.052695: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-16 11:41:41.401499: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-11-16 11:41:41.401519: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-11-16 11:41:42.615199: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-11-16 11:41:42.615371: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-11-16 11:41:42.615379: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import pandas as pd  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = r'/home/prashantb/Documents/Prashant/Thesis/FinalData'\n",
    "\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "test_dir = os.path.join(base_dir, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2  # 20% of training data used as validation data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13964 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Data generator for training data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    '/home/prashantb/Documents/Prashant/Thesis/FinalData/train',\n",
    "    target_size=(128, 128),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    subset='training'  # Set as training data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3490 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Data generator for validation data\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    '/home/prashantb/Documents/Prashant/Thesis/FinalData/train',\n",
    "    target_size=(128, 128),\n",
    "    batch_size=16,\n",
    "    class_mode='binary',\n",
    "    subset='validation'  # Set as validation data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2096 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Data generator for test data (no augmentation)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    '/home/prashantb/Documents/Prashant/Thesis/FinalData/test',\n",
    "    target_size=(128, 128),\n",
    "    batch_size=16,\n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import CSVLogger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dense, Dropout, BatchNormalization\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define the complex CNN model\n",
    "cnn_model = Sequential([\n",
    "    Conv2D(64, (3, 3), activation='relu', input_shape=(128, 128, 3), kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(64, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.1),\n",
    "\n",
    "    Conv2D(128, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(128, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.1),\n",
    "\n",
    "    Conv2D(256, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(256, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.1),\n",
    "\n",
    "    Conv2D(512, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(512, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.1),\n",
    "\n",
    "    GlobalAveragePooling2D(),\n",
    "\n",
    "    Dense(2048, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the CNN model\n",
    "cnn_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "log_file_path = 'cnn_training_log.csv'\n",
    "\n",
    "# Create CSVLogger callback\n",
    "csv_logger = CSVLogger(log_file_path, append=True)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "437/437 [==============================] - 1247s 3s/step - loss: 1.9961 - accuracy: 0.7520 - val_loss: 4.1895 - val_accuracy: 0.7146 - lr: 1.0000e-04\n",
      "Epoch 2/25\n",
      "437/437 [==============================] - 1167s 3s/step - loss: 1.7554 - accuracy: 0.7835 - val_loss: 1.8143 - val_accuracy: 0.7178 - lr: 1.0000e-04\n",
      "Epoch 3/25\n",
      "437/437 [==============================] - 1166s 3s/step - loss: 1.5261 - accuracy: 0.8053 - val_loss: 1.5377 - val_accuracy: 0.7607 - lr: 1.0000e-04\n",
      "Epoch 4/25\n",
      "437/437 [==============================] - 1188s 3s/step - loss: 1.3360 - accuracy: 0.8037 - val_loss: 1.2356 - val_accuracy: 0.8590 - lr: 1.0000e-04\n",
      "Epoch 5/25\n",
      "437/437 [==============================] - 1230s 3s/step - loss: 1.1563 - accuracy: 0.8177 - val_loss: 1.1260 - val_accuracy: 0.7877 - lr: 1.0000e-04\n",
      "Epoch 6/25\n",
      "437/437 [==============================] - 1233s 3s/step - loss: 1.0191 - accuracy: 0.8212 - val_loss: 0.8179 - val_accuracy: 0.9034 - lr: 1.0000e-04\n",
      "Epoch 7/25\n",
      "437/437 [==============================] - 1233s 3s/step - loss: 0.9033 - accuracy: 0.8261 - val_loss: 0.7438 - val_accuracy: 0.8937 - lr: 1.0000e-04\n",
      "Epoch 8/25\n",
      "437/437 [==============================] - 1261s 3s/step - loss: 0.8145 - accuracy: 0.8316 - val_loss: 0.6778 - val_accuracy: 0.8923 - lr: 1.0000e-04\n",
      "Epoch 9/25\n",
      "437/437 [==============================] - 1263s 3s/step - loss: 0.7441 - accuracy: 0.8309 - val_loss: 0.6748 - val_accuracy: 0.8321 - lr: 1.0000e-04\n",
      "Epoch 10/25\n",
      "437/437 [==============================] - 1287s 3s/step - loss: 0.6937 - accuracy: 0.8281 - val_loss: 0.5702 - val_accuracy: 0.8934 - lr: 1.0000e-04\n",
      "Epoch 11/25\n",
      "437/437 [==============================] - 1249s 3s/step - loss: 0.6443 - accuracy: 0.8362 - val_loss: 0.5132 - val_accuracy: 0.9037 - lr: 1.0000e-04\n",
      "Epoch 12/25\n",
      "437/437 [==============================] - 1255s 3s/step - loss: 0.6118 - accuracy: 0.8386 - val_loss: 0.4578 - val_accuracy: 0.9238 - lr: 1.0000e-04\n",
      "Epoch 13/25\n",
      "437/437 [==============================] - 1230s 3s/step - loss: 0.5882 - accuracy: 0.8376 - val_loss: 0.4922 - val_accuracy: 0.8897 - lr: 1.0000e-04\n",
      "Epoch 14/25\n",
      "437/437 [==============================] - 1231s 3s/step - loss: 0.5653 - accuracy: 0.8341 - val_loss: 0.3619 - val_accuracy: 0.9390 - lr: 1.0000e-04\n",
      "Epoch 15/25\n",
      "437/437 [==============================] - 1227s 3s/step - loss: 0.5402 - accuracy: 0.8418 - val_loss: 0.3105 - val_accuracy: 0.9533 - lr: 1.0000e-04\n",
      "Epoch 16/25\n",
      "437/437 [==============================] - 1231s 3s/step - loss: 0.5315 - accuracy: 0.8383 - val_loss: 0.3191 - val_accuracy: 0.9427 - lr: 1.0000e-04\n",
      "Epoch 17/25\n",
      "437/437 [==============================] - 1237s 3s/step - loss: 0.5111 - accuracy: 0.8429 - val_loss: 0.3629 - val_accuracy: 0.9264 - lr: 1.0000e-04\n",
      "Epoch 18/25\n",
      "437/437 [==============================] - 1227s 3s/step - loss: 0.5018 - accuracy: 0.8417 - val_loss: 0.3830 - val_accuracy: 0.9014 - lr: 1.0000e-04\n",
      "Epoch 19/25\n",
      "437/437 [==============================] - 1268s 3s/step - loss: 0.4869 - accuracy: 0.8424 - val_loss: 0.3177 - val_accuracy: 0.9355 - lr: 1.0000e-04\n",
      "Epoch 20/25\n",
      "437/437 [==============================] - 1323s 3s/step - loss: 0.4774 - accuracy: 0.8462 - val_loss: 0.2752 - val_accuracy: 0.9470 - lr: 1.0000e-04\n",
      "Epoch 21/25\n",
      "437/437 [==============================] - 1233s 3s/step - loss: 0.4717 - accuracy: 0.8433 - val_loss: 0.5497 - val_accuracy: 0.8077 - lr: 1.0000e-04\n",
      "Epoch 22/25\n",
      "437/437 [==============================] - 1231s 3s/step - loss: 0.4636 - accuracy: 0.8445 - val_loss: 0.3216 - val_accuracy: 0.9315 - lr: 1.0000e-04\n",
      "Epoch 23/25\n",
      "437/437 [==============================] - 1232s 3s/step - loss: 0.4557 - accuracy: 0.8472 - val_loss: 0.2497 - val_accuracy: 0.9499 - lr: 1.0000e-04\n",
      "Epoch 24/25\n",
      "437/437 [==============================] - 1236s 3s/step - loss: 0.4500 - accuracy: 0.8477 - val_loss: 0.3324 - val_accuracy: 0.9138 - lr: 1.0000e-04\n",
      "Epoch 25/25\n",
      "437/437 [==============================] - 1236s 3s/step - loss: 0.4470 - accuracy: 0.8445 - val_loss: 0.8065 - val_accuracy: 0.7255 - lr: 1.0000e-04\n",
      "219/219 [==============================] - 60s 267ms/step\n",
      "CNN Model - Accuracy: 0.5178\n",
      "CNN Model - Precision: 0.7149\n",
      "CNN Model - Recall: 0.5409\n",
      "CNN Model - F1-score: 0.6158\n",
      "CNN Model - Execution Time: 30923.21203136444 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Train the CNN model\n",
    "history_cnn = cnn_model.fit(\n",
    "    train_generator,\n",
    "    epochs=25,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[early_stopping, reduce_lr, csv_logger]\n",
    ")\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "execution_time_cnn = end_time - start_time\n",
    "\n",
    "# Log training and evaluation results\n",
    "logging.info(f'CNN Model - Training History: {history_cnn.history}')\n",
    "logging.info(f'Execution Time for CNN Model: {execution_time_cnn} seconds')\n",
    "\n",
    "# Evaluate the model on the validation data\n",
    "y_true = validation_generator.classes\n",
    "y_pred = (cnn_model.predict(validation_generator) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "# Log metrics\n",
    "logging.info(f'CNN Model - Accuracy: {accuracy:.4f}')\n",
    "logging.info(f'CNN Model - Precision: {precision:.4f}')\n",
    "logging.info(f'CNN Model - Recall: {recall:.4f}')\n",
    "logging.info(f'CNN Model - F1-score: {f1:.4f}')\n",
    "\n",
    "# Output results to the console\n",
    "print(f\"CNN Model - Accuracy: {accuracy:.4f}\")\n",
    "print(f\"CNN Model - Precision: {precision:.4f}\")\n",
    "print(f\"CNN Model - Recall: {recall:.4f}\")\n",
    "print(f\"CNN Model - F1-score: {f1:.4f}\")\n",
    "print(f\"CNN Model - Execution Time: {execution_time_cnn} seconds\")\n",
    "\n",
    "# Log the best achieved metric\n",
    "best_accuracy = max(history_cnn.history['val_accuracy'])\n",
    "best_precision = max(history_cnn.history['val_precision']) if 'val_precision' in history_cnn.history else precision\n",
    "best_recall = max(history_cnn.history['val_recall']) if 'val_recall' in history_cnn.history else recall\n",
    "best_f1 = max(history_cnn.history['val_f1_score']) if 'val_f1_score' in history_cnn.history else f1\n",
    "\n",
    "logging.info(f'Best CNN Model - Accuracy: {best_accuracy:.4f}')\n",
    "logging.info(f'Best CNN Model - Precision: {best_precision:.4f}')\n",
    "logging.info(f'Best CNN Model - Recall: {best_recall:.4f}')\n",
    "logging.info(f'Best CNN Model - F1-score: {best_f1:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
