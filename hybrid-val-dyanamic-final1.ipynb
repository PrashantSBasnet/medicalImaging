{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10006375,"sourceType":"datasetVersion","datasetId":6154221}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import (\n    Input, Conv2D, BatchNormalization, MaxPooling2D, \n    Dense, Dropout, Reshape, Lambda, Embedding, \n    MultiHeadAttention, LayerNormalization, \n    GlobalAveragePooling1D\n)\nfrom tensorflow.keras.models import Model\n\nimport os\nimport time\nimport logging\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# Set up logging\nlog_dir = '/kaggle/working/'\nos.makedirs(log_dir, exist_ok=True)\nlog_file = os.path.join(log_dir, 'hybrid_model_training.log')\nlogging.basicConfig(\n    filename=log_file,\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s'\n)\n\nclass Patches(layers.Layer):\n    def __init__(self, patch_size):\n        super().__init__()\n        self.patch_size = patch_size\n\n    def call(self, images):\n        batch_size = tf.shape(images)[0]\n        patches = tf.image.extract_patches(\n            images=images,\n            sizes=[1, self.patch_size, self.patch_size, 1],\n            strides=[1, self.patch_size, self.patch_size, 1],\n            rates=[1, 1, 1, 1],\n            padding=\"VALID\",\n        )\n        patch_dims = patches.shape[-1]\n        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n        return patches\n\nclass PatchEncoder(layers.Layer):\n    def __init__(self, num_patches, projection_dim):\n        super().__init__()\n        self.num_patches = num_patches\n        self.projection = layers.Dense(units=projection_dim)\n        self.position_embedding = layers.Embedding(\n            input_dim=num_patches, output_dim=projection_dim\n        )\n\n    def call(self, patch):\n        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n        encoded = self.projection(patch) + self.position_embedding(positions)\n        return encoded\n\ndef create_hybrid_vit_model(input_shape=(128, 128, 3), patch_size=16, num_patches=64, projection_dim=256, \n                             transformer_layers=4, num_heads=8, dropout_rate=0.1):\n    # Input layer\n    inputs = Input(shape=input_shape)\n    \n    # CNN Feature Extraction\n    x = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n    x = BatchNormalization()(x)\n    x = MaxPooling2D((2, 2))(x)\n    \n    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n    x = BatchNormalization()(x)\n    x = MaxPooling2D((2, 2))(x)\n    \n    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n    x = BatchNormalization()(x)\n    \n    # Ensure the output is 4D before patch extraction\n    x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n    \n    # Patch Extraction Layer\n    def extract_patches(x):\n        return tf.image.extract_patches(\n            images=x,\n            sizes=[1, patch_size, patch_size, 1],\n            strides=[1, patch_size, patch_size, 1],\n            rates=[1, 1, 1, 1],\n            padding=\"VALID\"\n        )\n    \n    patches = Lambda(extract_patches)(x)\n    \n    # Reshape patches\n    patch_dims = patches.shape[-1]\n    num_patches = (x.shape[1] // patch_size) * (x.shape[2] // patch_size)\n    patches = Reshape((-1, patch_dims))(patches)\n    \n    # Patch Embedding\n    x = Dense(projection_dim)(patches)\n    \n    # Add positional embeddings\n    positions = tf.range(start=0, limit=num_patches, delta=1)\n    position_embedding = Embedding(input_dim=num_patches, output_dim=projection_dim)(positions)\n    x = x + tf.expand_dims(position_embedding, axis=0)\n    \n    # Transformer Blocks\n    for _ in range(transformer_layers):\n        # Layer normalization and multi-head attention\n        x_norm = LayerNormalization(epsilon=1e-6)(x)\n        attention_output = MultiHeadAttention(\n            num_heads=num_heads, \n            key_dim=projection_dim // num_heads\n        )(x_norm, x_norm)\n        x = x + attention_output\n        \n        # MLP\n        mlp_norm = LayerNormalization(epsilon=1e-6)(x)\n        mlp = Dense(projection_dim * 2, activation='gelu')(mlp_norm)\n        mlp = Dropout(dropout_rate)(mlp)\n        mlp = Dense(projection_dim)(mlp)\n        x = x + mlp\n    \n    # Global average pooling and classification\n    x = LayerNormalization(epsilon=1e-6)(x)\n    x = GlobalAveragePooling1D()(x)\n    x = Dropout(dropout_rate)(x)\n    \n    # Final classification layer\n    outputs = Dense(1, activation='sigmoid')(x)\n    \n    model = Model(inputs=inputs, outputs=outputs)\n    return model\n\ndef load_and_preprocess_data(data_dir):\n    \"\"\"\n    Load images and labels from training directory\n    Returns X (images), y (labels)\n    \"\"\"\n    images = []\n    labels = []\n    \n    for label, class_name in enumerate(['NORMAL', 'PNEUMONIA']):\n        class_dir = os.path.join(data_dir, class_name)\n        for img_name in os.listdir(class_dir):\n            img_path = os.path.join(class_dir, img_name)\n            \n            # Read and resize image\n            img = tf.keras.preprocessing.image.load_img(img_path, target_size=(128, 128))\n            img_array = tf.keras.preprocessing.image.img_to_array(img)\n            img_array = img_array / 255.0  # Normalize\n            \n            images.append(img_array)\n            labels.append(label)\n    \n    return np.array(images), np.array(labels)\n\ndef create_dynamic_validation_generator(X_train, y_train, batch_size=32):\n    \"\"\"\n    Create a dynamic validation generator using train_test_split\n    \"\"\"\n    # Split training data into training and validation sets\n    X_train, X_val, y_train, y_val = train_test_split(\n        X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n    )\n    \n    # Data augmentation for training\n    train_datagen = ImageDataGenerator(\n        rotation_range=20,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True,\n        fill_mode='nearest'\n    )\n    \n    # No augmentation for validation\n    val_datagen = ImageDataGenerator()\n    \n    # Create generators\n    train_generator = train_datagen.flow(\n        X_train, y_train, \n        batch_size=batch_size, \n        shuffle=True\n    )\n    \n    val_generator = val_datagen.flow(\n        X_val, y_val, \n        batch_size=batch_size, \n        shuffle=False\n    )\n    \n    return train_generator, val_generator\n\ndef train_model(model, train_generator, validation_generator, epochs=30):\n    optimizer = tf.keras.optimizers.AdamW(\n        learning_rate=3e-4,\n        weight_decay=1e-4\n    )\n    \n    model.compile(\n        optimizer=optimizer,\n        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.1),\n        metrics=[\"accuracy\"],\n    )\n    \n    callbacks = [\n        keras.callbacks.EarlyStopping(\n            monitor=\"val_loss\",\n            patience=5,\n            restore_best_weights=True\n        ),\n        keras.callbacks.ReduceLROnPlateau(\n            monitor=\"val_loss\",\n            factor=0.2,\n            patience=3,\n            min_lr=1e-6\n        ),\n        keras.callbacks.ModelCheckpoint(\n            'best_hybrid_model.keras',\n            monitor=\"val_loss\",\n            save_best_only=True\n        ),\n        keras.callbacks.CSVLogger(\n            os.path.join(log_dir, 'vit_training.csv')\n        )\n    ]\n    \n    history = model.fit(\n        train_generator,\n        validation_data=validation_generator,\n        epochs=epochs,\n        callbacks=callbacks,\n        verbose=1\n    )\n    \n    return history\n\ndef main():\n    # Load and preprocess training data\n    data_dir = '/kaggle/input/chest-x-ray-images/FinalData/train'\n    X, y = load_and_preprocess_data(data_dir)\n    \n    # Create dynamic validation generator\n    train_generator, validation_generator = create_dynamic_validation_generator(X, y)\n    \n    # Create and train model\n    vit_model = create_hybrid_vit_model()\n    start_time = time.time()\n    history = train_model(vit_model, train_generator, validation_generator)\n    execution_time = time.time() - start_time\n\n    # Evaluate model\n    y_true = validation_generator.y\n    y_pred = (vit_model.predict(validation_generator) > 0.5).astype(\"int32\")\n\n    # Compute metrics\n    accuracy = accuracy_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred)\n    recall = recall_score(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred)\n\n    # Log and print results\n    logging.info(f'Hybrid Model - Accuracy: {accuracy:.4f}')\n    logging.info(f'Hybrid Model - Precision: {precision:.4f}')\n    logging.info(f'Hybrid Model - Recall: {recall:.4f}')\n    logging.info(f'Hybrid Model - F1-score: {f1:.4f}')\n    logging.info(f'Hybrid Model - Execution Time: {execution_time:.2f} seconds')\n\n    print(f\"Hybrid Model - Accuracy: {accuracy:.4f}\")\n    print(f\"Hybrid Model - Precision: {precision:.4f}\")\n    print(f\"Hybrid Model - Recall: {recall:.4f}\")\n    print(f\"Hybrid Model - F1-score: {f1:.4f}\")\n    print(f\"Hybrid Model - Execution Time: {execution_time:.2f} seconds\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-27T16:18:36.003732Z","iopub.execute_input":"2024-11-27T16:18:36.004072Z","iopub.status.idle":"2024-11-27T16:35:25.142286Z","shell.execute_reply.started":"2024-11-27T16:18:36.004042Z","shell.execute_reply":"2024-11-27T16:35:25.141411Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/30\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1732724429.467881     145 service.cc:145] XLA service 0x793d7800f960 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1732724429.467940     145 service.cc:153]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1732724429.467945     145 service.cc:153]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1732724448.986627     196 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_27', 768 bytes spill stores, 768 bytes spill loads\n\nI0000 00:00:1732724450.139909     198 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_25', 768 bytes spill stores, 768 bytes spill loads\n\nI0000 00:00:1732724452.062212     199 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_49', 768 bytes spill stores, 768 bytes spill loads\n\nI0000 00:00:1732724456.672065     196 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_177', 256 bytes spill stores, 256 bytes spill loads\n\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m  2/433\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 56ms/step - accuracy: 0.6797 - loss: 1.3093   ","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1732724470.677346     145 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m281/433\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 113ms/step - accuracy: 0.7227 - loss: 0.6486","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1732724510.184147     267 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_177', 16 bytes spill stores, 16 bytes spill loads\n\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m432/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.7360 - loss: 0.6131","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1732724547.700535     326 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_31', 256 bytes spill stores, 256 bytes spill loads\n\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 193ms/step - accuracy: 0.7361 - loss: 0.6128 - val_accuracy: 0.7117 - val_loss: 0.8197 - learning_rate: 3.0000e-04\nEpoch 2/30\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 120ms/step - accuracy: 0.8155 - loss: 0.4714 - val_accuracy: 0.8091 - val_loss: 0.5151 - learning_rate: 3.0000e-04\nEpoch 3/30\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 115ms/step - accuracy: 0.8321 - loss: 0.4523 - val_accuracy: 0.6958 - val_loss: 0.6087 - learning_rate: 3.0000e-04\nEpoch 4/30\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 117ms/step - accuracy: 0.8396 - loss: 0.4416 - val_accuracy: 0.6831 - val_loss: 0.6571 - learning_rate: 3.0000e-04\nEpoch 5/30\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 117ms/step - accuracy: 0.8377 - loss: 0.4423 - val_accuracy: 0.7955 - val_loss: 0.5076 - learning_rate: 3.0000e-04\nEpoch 6/30\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 116ms/step - accuracy: 0.8472 - loss: 0.4318 - val_accuracy: 0.5466 - val_loss: 1.0050 - learning_rate: 3.0000e-04\nEpoch 7/30\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 116ms/step - accuracy: 0.8465 - loss: 0.4237 - val_accuracy: 0.6776 - val_loss: 0.6730 - learning_rate: 3.0000e-04\nEpoch 8/30\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 118ms/step - accuracy: 0.8564 - loss: 0.4127 - val_accuracy: 0.8126 - val_loss: 0.5025 - learning_rate: 3.0000e-04\nEpoch 9/30\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 115ms/step - accuracy: 0.8570 - loss: 0.4147 - val_accuracy: 0.7371 - val_loss: 0.5729 - learning_rate: 3.0000e-04\nEpoch 10/30\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 115ms/step - accuracy: 0.8562 - loss: 0.4138 - val_accuracy: 0.7189 - val_loss: 0.5615 - learning_rate: 3.0000e-04\nEpoch 11/30\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 117ms/step - accuracy: 0.8590 - loss: 0.4061 - val_accuracy: 0.8600 - val_loss: 0.4125 - learning_rate: 3.0000e-04\nEpoch 12/30\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 115ms/step - accuracy: 0.8586 - loss: 0.4108 - val_accuracy: 0.8103 - val_loss: 0.4902 - learning_rate: 3.0000e-04\nEpoch 13/30\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 115ms/step - accuracy: 0.8642 - loss: 0.4051 - val_accuracy: 0.8528 - val_loss: 0.4150 - learning_rate: 3.0000e-04\nEpoch 14/30\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 115ms/step - accuracy: 0.8614 - loss: 0.4060 - val_accuracy: 0.6209 - val_loss: 0.7652 - learning_rate: 3.0000e-04\nEpoch 15/30\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 115ms/step - accuracy: 0.8602 - loss: 0.4052 - val_accuracy: 0.8276 - val_loss: 0.4887 - learning_rate: 6.0000e-05\nEpoch 16/30\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 116ms/step - accuracy: 0.8672 - loss: 0.3986 - val_accuracy: 0.8436 - val_loss: 0.4409 - learning_rate: 6.0000e-05\n\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 26ms/step\nHybrid Model - Accuracy: 0.8600\nHybrid Model - Precision: 0.8547\nHybrid Model - Recall: 0.9679\nHybrid Model - F1-score: 0.9078\nHybrid Model - Execution Time: 904.11 seconds\n","output_type":"stream"}],"execution_count":5}]}