{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 19:59:20.127461: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-15 19:59:20.588026: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-11-15 19:59:20.588044: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-11-15 19:59:22.055062: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-11-15 19:59:22.055151: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-11-15 19:59:22.055159: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.11.0\n",
      "TensorFlow Addons version: 0.19.0\n",
      "NumPy version: 1.23.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prashantb/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers  # Ensure this line is included\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"TensorFlow Addons version:\", tfa.__version__)\n",
    "print(\"NumPy version:\", np.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models\n",
    "import os\n",
    "import pandas as pd  \n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13964 images belonging to 2 classes.\n",
      "Found 3490 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Set up logging to save the output\n",
    "logging.basicConfig(filename='model_comparison.log', level=logging.INFO)\n",
    "\n",
    "# Data Augmentation for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2  # 20% of training data used as validation data\n",
    ")\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Data generators for training and validation data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    '/home/prashantb/Documents/Prashant/Thesis/FinalData/train',\n",
    "    target_size=(128, 128),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    subset='training'  # Set as training data\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    '/home/prashantb/Documents/Prashant/Thesis/FinalData/train',\n",
    "    target_size=(128, 128),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    subset='validation'  # Set as validation data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 19:59:32.071144: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2024-11-15 19:59:32.071166: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2024-11-15 19:59:32.071182: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (prashantB-viveka): /proc/driver/nvidia/version does not exist\n",
      "2024-11-15 19:59:32.071776: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Vision Transformer parameters\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "mlp_dim = 2048\n",
    "hidden_dim = 512\n",
    "patch_size = 16\n",
    "num_patches = (128 // patch_size) ** 2  # (128 / 16)^2 = 64 patches\n",
    "dropout_rate = 0.1\n",
    "\n",
    "# Patch and Position Embedding Layer\n",
    "class PatchEmbedding(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded\n",
    "\n",
    "# Transformer Block\n",
    "def transformer_block(inputs, num_heads, mlp_dim, dropout_rate):\n",
    "    x1 = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    attention_output = layers.MultiHeadAttention(\n",
    "        num_heads=num_heads, key_dim=hidden_dim, dropout=dropout_rate\n",
    "    )(x1, x1)\n",
    "    x2 = layers.Add()([attention_output, inputs])\n",
    "\n",
    "    x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "    x3 = layers.Dense(mlp_dim, activation=tf.nn.gelu)(x3)\n",
    "    x3 = layers.Dropout(dropout_rate)(x3)\n",
    "    x3 = layers.Dense(hidden_dim)(x3)\n",
    "    return layers.Add()([x3, x2])\n",
    "\n",
    "# Vision Transformer Model\n",
    "def create_vit_classifier():\n",
    "    inputs = layers.Input(shape=(128, 128, 3))\n",
    "    patches = layers.Conv2D(hidden_dim, kernel_size=patch_size, strides=patch_size)(inputs)\n",
    "    patches = layers.Reshape((num_patches, hidden_dim))(patches)\n",
    "    encoded_patches = PatchEmbedding(num_patches, hidden_dim)(patches)\n",
    "\n",
    "    for _ in range(num_layers):\n",
    "        encoded_patches = transformer_block(encoded_patches, num_heads, mlp_dim, dropout_rate)\n",
    "\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "    features = layers.Dense(mlp_dim, activation=tf.nn.gelu)(representation)\n",
    "    logits = layers.Dense(1)(features)\n",
    "    outputs = layers.Activation(\"sigmoid\")(logits)\n",
    "    return models.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n",
    "# Create and compile the ViT model\n",
    "vit_model = create_vit_classifier()\n",
    "vit_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, classification_report\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "437/437 [==============================] - 2188s 5s/step - loss: 0.7254 - accuracy: 0.6939 - val_loss: 0.5177 - val_accuracy: 0.7298 - lr: 1.0000e-04\n",
      "Epoch 2/25\n",
      "437/437 [==============================] - 2168s 5s/step - loss: 0.5951 - accuracy: 0.7080 - val_loss: 0.5568 - val_accuracy: 0.7433 - lr: 1.0000e-04\n",
      "Epoch 3/25\n",
      "437/437 [==============================] - 2098s 5s/step - loss: 0.5763 - accuracy: 0.7214 - val_loss: 0.4756 - val_accuracy: 0.7722 - lr: 1.0000e-04\n",
      "Epoch 4/25\n",
      "437/437 [==============================] - 2095s 5s/step - loss: 0.5566 - accuracy: 0.7288 - val_loss: 0.4247 - val_accuracy: 0.8160 - lr: 1.0000e-04\n",
      "Epoch 5/25\n",
      "437/437 [==============================] - 2108s 5s/step - loss: 0.5407 - accuracy: 0.7349 - val_loss: 0.4812 - val_accuracy: 0.7593 - lr: 1.0000e-04\n",
      "Epoch 6/25\n",
      "437/437 [==============================] - 2124s 5s/step - loss: 0.5286 - accuracy: 0.7420 - val_loss: 0.3939 - val_accuracy: 0.8241 - lr: 1.0000e-04\n",
      "Epoch 7/25\n",
      "437/437 [==============================] - 2096s 5s/step - loss: 0.5213 - accuracy: 0.7393 - val_loss: 0.4620 - val_accuracy: 0.7639 - lr: 1.0000e-04\n",
      "Epoch 8/25\n",
      "437/437 [==============================] - 2098s 5s/step - loss: 0.5026 - accuracy: 0.7528 - val_loss: 0.3801 - val_accuracy: 0.8284 - lr: 1.0000e-04\n",
      "Epoch 9/25\n",
      "437/437 [==============================] - 2129s 5s/step - loss: 0.4876 - accuracy: 0.7585 - val_loss: 0.3592 - val_accuracy: 0.8487 - lr: 1.0000e-04\n",
      "Epoch 10/25\n",
      "437/437 [==============================] - 2093s 5s/step - loss: 0.4882 - accuracy: 0.7600 - val_loss: 0.3625 - val_accuracy: 0.8470 - lr: 1.0000e-04\n",
      "Epoch 11/25\n",
      "437/437 [==============================] - 2092s 5s/step - loss: 0.4794 - accuracy: 0.7634 - val_loss: 0.3378 - val_accuracy: 0.8521 - lr: 1.0000e-04\n",
      "Epoch 12/25\n",
      "437/437 [==============================] - 2097s 5s/step - loss: 0.4748 - accuracy: 0.7662 - val_loss: 0.3357 - val_accuracy: 0.8533 - lr: 1.0000e-04\n",
      "Epoch 13/25\n",
      "437/437 [==============================] - 2093s 5s/step - loss: 0.4632 - accuracy: 0.7711 - val_loss: 0.3630 - val_accuracy: 0.8407 - lr: 1.0000e-04\n",
      "Epoch 14/25\n",
      "437/437 [==============================] - 2092s 5s/step - loss: 0.4631 - accuracy: 0.7751 - val_loss: 0.3511 - val_accuracy: 0.8553 - lr: 1.0000e-04\n",
      "Epoch 15/25\n",
      "437/437 [==============================] - 2092s 5s/step - loss: 0.4579 - accuracy: 0.7728 - val_loss: 0.3311 - val_accuracy: 0.8751 - lr: 1.0000e-04\n",
      "Epoch 16/25\n",
      "437/437 [==============================] - 2091s 5s/step - loss: 0.4500 - accuracy: 0.7808 - val_loss: 0.3132 - val_accuracy: 0.8673 - lr: 1.0000e-04\n",
      "Epoch 17/25\n",
      "437/437 [==============================] - 2093s 5s/step - loss: 0.4454 - accuracy: 0.7834 - val_loss: 0.3029 - val_accuracy: 0.8759 - lr: 1.0000e-04\n",
      "Epoch 18/25\n",
      "437/437 [==============================] - 2097s 5s/step - loss: 0.4496 - accuracy: 0.7786 - val_loss: 0.3479 - val_accuracy: 0.8602 - lr: 1.0000e-04\n",
      "Epoch 19/25\n",
      "437/437 [==============================] - 2096s 5s/step - loss: 0.4410 - accuracy: 0.7851 - val_loss: 0.2908 - val_accuracy: 0.8788 - lr: 1.0000e-04\n",
      "Epoch 20/25\n",
      "437/437 [==============================] - 2094s 5s/step - loss: 0.4380 - accuracy: 0.7903 - val_loss: 0.2966 - val_accuracy: 0.8742 - lr: 1.0000e-04\n",
      "Epoch 21/25\n",
      "437/437 [==============================] - 2087s 5s/step - loss: 0.4323 - accuracy: 0.7938 - val_loss: 0.2856 - val_accuracy: 0.8785 - lr: 1.0000e-04\n",
      "Epoch 22/25\n",
      "437/437 [==============================] - 2088s 5s/step - loss: 0.4291 - accuracy: 0.7930 - val_loss: 0.2754 - val_accuracy: 0.8831 - lr: 1.0000e-04\n",
      "Epoch 23/25\n",
      "437/437 [==============================] - 2091s 5s/step - loss: 0.4292 - accuracy: 0.7900 - val_loss: 0.2786 - val_accuracy: 0.8888 - lr: 1.0000e-04\n",
      "Epoch 24/25\n",
      "437/437 [==============================] - 2092s 5s/step - loss: 0.4344 - accuracy: 0.7888 - val_loss: 0.2601 - val_accuracy: 0.8897 - lr: 1.0000e-04\n",
      "Epoch 25/25\n",
      "437/437 [==============================] - 2098s 5s/step - loss: 0.4268 - accuracy: 0.7922 - val_loss: 0.2766 - val_accuracy: 0.8782 - lr: 1.0000e-04\n",
      "110/110 [==============================] - 141s 1s/step\n",
      "ViT Model - Accuracy: 0.5702\n",
      "ViT Model - Precision: 0.7037\n",
      "ViT Model - Recall: 0.6885\n",
      "ViT Model - F1-score: 0.6960\n",
      "ViT Model - Execution Time: 52594.05940413475 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "history_vit = vit_model.fit(\n",
    "    train_generator,\n",
    "    epochs=25,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_vit = end_time - start_time\n",
    "\n",
    "# Log training and evaluation results\n",
    "logging.info(f'ViT Model - Training History: {history_vit.history}')\n",
    "logging.info(f'Execution Time for ViT Model: {execution_time_vit} seconds')\n",
    "\n",
    "# Evaluate the model on the validation data\n",
    "y_true = validation_generator.classes\n",
    "y_pred = (vit_model.predict(validation_generator) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "# Log metrics\n",
    "logging.info(f'ViT Model - Accuracy: {accuracy:.4f}')\n",
    "logging.info(f'ViT Model - Precision: {precision:.4f}')\n",
    "logging.info(f'ViT Model - Recall: {recall:.4f}')\n",
    "logging.info(f'ViT Model - F1-score: {f1:.4f}')\n",
    "\n",
    "# Output results to the console\n",
    "print(f\"ViT Model - Accuracy: {accuracy:.4f}\")\n",
    "print(f\"ViT Model - Precision: {precision:.4f}\")\n",
    "print(f\"ViT Model - Recall: {recall:.4f}\")\n",
    "print(f\"ViT Model - F1-score: {f1:.4f}\")\n",
    "print(f\"ViT Model - Execution Time: {execution_time_vit} seconds\")\n",
    "\n",
    "y_pred = (vit_model.predict(validation_generator) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "# Log metrics\n",
    "logging.info(f'ViT Model - Accuracy: {accuracy:.4f}')\n",
    "logging.info(f'ViT Model - Precision: {precision:.4f}')\n",
    "logging.info(f'ViT Model - Recall: {recall:.4f}')\n",
    "logging.info(f'ViT Model - F1-score: {f1:.4f}')\n",
    "\n",
    "# Output results to the console\n",
    "print(f\"ViT Model - Accuracy: {accuracy:.4f}\")\n",
    "print(f\"ViT Model - Precision: {precision:.4f}\")\n",
    "print(f\"ViT Model - Recall: {recall:.4f}\")\n",
    "print(f\"ViT Model - F1-score: {f1:.4f}\")\n",
    "print(f\"ViT Model - Execution Time: {execution_time_vit} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
