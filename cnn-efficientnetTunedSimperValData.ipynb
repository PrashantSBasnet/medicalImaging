{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10006375,"sourceType":"datasetVersion","datasetId":6154221}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport time\nimport logging\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dense, Dropout\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, CSVLogger\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.utils.class_weight import compute_class_weight\nimport sys\n\n\n# Set up logging\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nlog_dir = '/kaggle/working/'\nos.makedirs(log_dir, exist_ok=True)\nlog_file = os.path.join(log_dir, 'efficientnet_model_training.log')\nlogging.basicConfig(\n    filename=log_file,\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s'\n)\n\n\n# Redirect output to a log file\nlog_file = \"/kaggle/working/training_output.log\"\nsys.stdout = open(log_file, \"w\")\n\n# Your training code\nhistory = model.fit(\n    train_data,\n    validation_data=val_data,\n    epochs=25,\n    verbose=1\n)\n\n# Restore standard output\nsys.stdout.close()\nsys.stdout = sys.__stdout__\n\n\ndef create_efficient_net_model(input_shape=(128, 128, 3)):\n    # Load pre-trained EfficientNetB0 with imagenet weights\n    base_model = EfficientNetB0(\n        weights='imagenet',\n        include_top=False,\n        input_shape=input_shape\n    )\n    \n    # Freeze the base model layers\n    base_model.trainable = False\n    \n    # Create new model\n    inputs = Input(shape=input_shape)\n    # Preprocess input\n    x = tf.keras.applications.efficientnet.preprocess_input(inputs)\n    \n    # Base model\n    x = base_model(x)\n    \n    # Add custom layers\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(512, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    outputs = Dense(1, activation='sigmoid')(x)\n    \n    model = Model(inputs, outputs)\n    return model, base_model\n\n# Data augmentation setup\ntrain_datagen = ImageDataGenerator(\n    preprocessing_function=tf.keras.applications.efficientnet.preprocess_input,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest',\n    validation_split=0.2\n)\n\ntest_datagen = ImageDataGenerator(\n    preprocessing_function=tf.keras.applications.efficientnet.preprocess_input\n)\n\ndef train_model(model, base_model, train_generator, validation_generator, epochs=50):\n    # First phase: Train only the top layers\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.1),\n        metrics=['accuracy']\n    )\n    \n    callbacks = [\n        EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n        ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6),\n        ModelCheckpoint('best_efficientnet_model.keras', monitor='val_loss', save_best_only=True),\n        CSVLogger(os.path.join(log_dir, 'efficientnet_training.csv'))\n    ]\n    \n    # First phase training\n    history_1 = model.fit(\n        train_generator,\n        validation_data=validation_generator,\n        epochs=epochs//2,\n        callbacks=callbacks\n    )\n    \n    # Second phase: Fine-tune the last few layers of the base model\n    # Unfreeze the last 20 layers\n    base_model.trainable = True\n    for layer in base_model.layers[:-20]:\n        layer.trainable = False\n    \n    # Recompile with lower learning rate\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.1),\n        metrics=['accuracy']\n    )\n    \n    # Second phase training\n    history_2 = model.fit(\n        train_generator,\n        validation_data=validation_generator,\n        epochs=epochs//2,\n        callbacks=callbacks\n    )\n    \n    return history_1, history_2\n\n# Data generators setup\ntrain_generator = train_datagen.flow_from_directory(\n    '/kaggle/input/chest-x-ray-images/FinalData/train',\n    target_size=(128, 128),\n    batch_size=32,\n    class_mode='binary',\n    subset='training'\n)\n\nvalidation_generator = train_datagen.flow_from_directory(\n    '/kaggle/input/chest-x-ray-images/FinalData/val',\n    target_size=(128, 128),\n    batch_size=32,\n    class_mode='binary',\n    subset='validation'\n)\n\ntest_generator = test_datagen.flow_from_directory(\n    '/kaggle/input/chest-x-ray-images/FinalData/test',\n    target_size=(128, 128),\n    batch_size=32,\n    class_mode='binary'\n)\n\n# Create and train model\nmodel, base_model = create_efficient_net_model()\nstart_time = time.time()\nhistory_1, history_2 = train_model(model, base_model, train_generator, validation_generator)\nexecution_time = time.time() - start_time\n\n# Evaluate model\ny_true = validation_generator.classes\ny_pred = (model.predict(validation_generator) > 0.5).astype(\"int32\")\n\n# Compute metrics\naccuracy = accuracy_score(y_true, y_pred)\nprecision = precision_score(y_true, y_pred)\nrecall = recall_score(y_true, y_pred)\nf1 = f1_score(y_true, y_pred)\n\n# Log and print results\nlogging.info(f'EfficientNet Model - Accuracy: {accuracy:.4f}')\nlogging.info(f'EfficientNet Model - Precision: {precision:.4f}')\nlogging.info(f'EfficientNet Model - Recall: {recall:.4f}')\nlogging.info(f'EfficientNet Model - F1-score: {f1:.4f}')\nlogging.info(f'EfficientNet Model - Execution Time: {execution_time:.2f} seconds')\n\nprint(f\"EfficientNet Model - Accuracy: {accuracy:.4f}\")\nprint(f\"EfficientNet Model - Precision: {precision:.4f}\")\nprint(f\"EfficientNet Model - Recall: {recall:.4f}\")\nprint(f\"EfficientNet Model - F1-score: {f1:.4f}\")\nprint(f\"EfficientNet Model - Execution Time: {execution_time:.2f} seconds\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-25T09:43:46.291343Z","iopub.execute_input":"2024-11-25T09:43:46.291711Z","iopub.status.idle":"2024-11-25T10:30:21.737251Z","shell.execute_reply.started":"2024-11-25T09:43:46.291680Z","shell.execute_reply":"2024-11-25T10:30:21.736473Z"}},"outputs":[{"name":"stdout","text":"Found 13830 images belonging to 2 classes.\nFound 166 images belonging to 2 classes.\nFound 2096 images belonging to 2 classes.\nDownloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n\u001b[1m16705208/16705208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\nEpoch 1/15\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1732527885.944043     118 service.cc:145] XLA service 0x7e54fc0029c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1732527885.944114     118 service.cc:153]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1732527885.944118     118 service.cc:153]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m  1/433\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:04:45\u001b[0m 34s/step - accuracy: 0.5312 - loss: 0.6887","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1732527907.075251     118 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m239s\u001b[0m 475ms/step - accuracy: 0.7579 - loss: 0.5472 - val_accuracy: 0.8735 - val_loss: 0.4031 - learning_rate: 0.0010\nEpoch 2/15\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 266ms/step - accuracy: 0.8204 - loss: 0.4738 - val_accuracy: 0.8313 - val_loss: 0.4294 - learning_rate: 0.0010\nEpoch 3/15\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 266ms/step - accuracy: 0.8201 - loss: 0.4707 - val_accuracy: 0.9277 - val_loss: 0.3863 - learning_rate: 0.0010\nEpoch 4/15\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 264ms/step - accuracy: 0.8314 - loss: 0.4539 - val_accuracy: 0.9096 - val_loss: 0.3532 - learning_rate: 0.0010\nEpoch 5/15\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 264ms/step - accuracy: 0.8324 - loss: 0.4509 - val_accuracy: 0.9277 - val_loss: 0.3573 - learning_rate: 0.0010\nEpoch 6/15\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 263ms/step - accuracy: 0.8385 - loss: 0.4450 - val_accuracy: 0.9036 - val_loss: 0.3579 - learning_rate: 0.0010\nEpoch 7/15\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 262ms/step - accuracy: 0.8424 - loss: 0.4400 - val_accuracy: 0.9217 - val_loss: 0.3299 - learning_rate: 0.0010\nEpoch 8/15\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 260ms/step - accuracy: 0.8409 - loss: 0.4386 - val_accuracy: 0.8976 - val_loss: 0.3778 - learning_rate: 0.0010\nEpoch 9/15\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 267ms/step - accuracy: 0.8323 - loss: 0.4456 - val_accuracy: 0.9157 - val_loss: 0.3500 - learning_rate: 0.0010\nEpoch 10/15\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 270ms/step - accuracy: 0.8360 - loss: 0.4447 - val_accuracy: 0.9277 - val_loss: 0.3381 - learning_rate: 0.0010\nEpoch 11/15\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 267ms/step - accuracy: 0.8389 - loss: 0.4378 - val_accuracy: 0.9096 - val_loss: 0.3562 - learning_rate: 2.0000e-04\nEpoch 12/15\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 264ms/step - accuracy: 0.8467 - loss: 0.4313 - val_accuracy: 0.8976 - val_loss: 0.3622 - learning_rate: 2.0000e-04\nEpoch 1/15\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 326ms/step - accuracy: 0.8129 - loss: 0.4790 - val_accuracy: 0.9337 - val_loss: 0.3504 - learning_rate: 1.0000e-04\nEpoch 2/15\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 269ms/step - accuracy: 0.8401 - loss: 0.4452 - val_accuracy: 0.9096 - val_loss: 0.3532 - learning_rate: 1.0000e-04\nEpoch 3/15\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 264ms/step - accuracy: 0.8420 - loss: 0.4360 - val_accuracy: 0.9337 - val_loss: 0.3252 - learning_rate: 1.0000e-04\nEpoch 4/15\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 266ms/step - accuracy: 0.8400 - loss: 0.4352 - val_accuracy: 0.9337 - val_loss: 0.3209 - learning_rate: 1.0000e-04\nEpoch 5/15\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 264ms/step - accuracy: 0.8587 - loss: 0.4176 - val_accuracy: 0.9518 - val_loss: 0.3182 - learning_rate: 1.0000e-04\nEpoch 6/15\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 262ms/step - accuracy: 0.8538 - loss: 0.4192 - val_accuracy: 0.9458 - val_loss: 0.3437 - learning_rate: 1.0000e-04\nEpoch 7/15\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 260ms/step - accuracy: 0.8590 - loss: 0.4134 - val_accuracy: 0.9337 - val_loss: 0.3307 - learning_rate: 1.0000e-04\nEpoch 8/15\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 259ms/step - accuracy: 0.8550 - loss: 0.4141 - val_accuracy: 0.9157 - val_loss: 0.3632 - learning_rate: 1.0000e-04\nEpoch 9/15\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 260ms/step - accuracy: 0.8596 - loss: 0.4086 - val_accuracy: 0.9458 - val_loss: 0.3245 - learning_rate: 2.0000e-05\nEpoch 10/15\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 257ms/step - accuracy: 0.8626 - loss: 0.4091 - val_accuracy: 0.9277 - val_loss: 0.3506 - learning_rate: 2.0000e-05\n\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step \nEfficientNet Model - Accuracy: 0.5482\nEfficientNet Model - Precision: 0.6635\nEfficientNet Model - Recall: 0.6330\nEfficientNet Model - F1-score: 0.6479\nEfficientNet Model - Execution Time: 2738.50 seconds\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport time\nimport logging\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dense, Dropout\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, CSVLogger\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sys\nimport zipfile\nimport pandas as pd\nfrom datetime import datetime\n\n# Create timestamp for unique run identification\ntimestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n\n# Set up directories\nbase_log_dir = '/kaggle/working/model_logs'\nrun_dir = os.path.join(base_log_dir, f'run_{timestamp}')\nmodel_dir = os.path.join(run_dir, 'models')\nplot_dir = os.path.join(run_dir, 'plots')\nlog_dir = os.path.join(run_dir, 'logs')\n\n# Create directories\nfor dir_path in [model_dir, plot_dir, log_dir]:\n    os.makedirs(dir_path, exist_ok=True)\n\n# Set up logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler(os.path.join(log_dir, 'training.log')),\n        logging.StreamHandler(sys.stdout)\n    ]\n)\n\ndef plot_training_history(history1, history2, plot_dir):\n    \"\"\"Plot and save training metrics\"\"\"\n    # Combine histories\n    history_combined = {\n        'accuracy': history1.history['accuracy'] + history2.history['accuracy'],\n        'val_accuracy': history1.history['val_accuracy'] + history2.history['val_accuracy'],\n        'loss': history1.history['loss'] + history2.history['loss'],\n        'val_loss': history1.history['val_loss'] + history2.history['val_loss']\n    }\n    \n    # Plot training curves\n    plt.figure(figsize=(15, 5))\n    \n    # Accuracy plot\n    plt.subplot(1, 2, 1)\n    plt.plot(history_combined['accuracy'], label='Training Accuracy')\n    plt.plot(history_combined['val_accuracy'], label='Validation Accuracy')\n    plt.title('Model Accuracy over Epochs')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.grid(True)\n    \n    # Loss plot\n    plt.subplot(1, 2, 2)\n    plt.plot(history_combined['loss'], label='Training Loss')\n    plt.plot(history_combined['val_loss'], label='Validation Loss')\n    plt.title('Model Loss over Epochs')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.grid(True)\n    \n    plt.tight_layout()\n    plt.savefig(os.path.join(plot_dir, 'training_history.png'))\n    plt.close()\n\ndef plot_confusion_matrix(y_true, y_pred, plot_dir):\n    \"\"\"Plot and save confusion matrix\"\"\"\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n    plt.title('Confusion Matrix')\n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    plt.savefig(os.path.join(plot_dir, 'confusion_matrix.png'))\n    plt.close()\n\ndef save_metrics_report(metrics, log_dir):\n    \"\"\"Save detailed metrics report\"\"\"\n    with open(os.path.join(log_dir, 'metrics_report.txt'), 'w') as f:\n        for key, value in metrics.items():\n            f.write(f\"{key}: {value}\\n\")\n\ndef create_efficient_net_model(input_shape=(128, 128, 3)):\n    # Load pre-trained EfficientNetB0 with imagenet weights\n    base_model = EfficientNetB0(\n        weights='imagenet',\n        include_top=False,\n        input_shape=input_shape\n    )\n    \n    # Freeze the base model layers\n    base_model.trainable = False\n    \n    # Create new model\n    inputs = Input(shape=input_shape)\n    # Preprocess input\n    x = tf.keras.applications.efficientnet.preprocess_input(inputs)\n    \n    # Base model\n    x = base_model(x)\n    \n    # Add custom layers\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(512, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    outputs = Dense(1, activation='sigmoid')(x)\n    \n    model = Model(inputs, outputs)\n    return model, base_model\n\n# Data augmentation setup\ntrain_datagen = ImageDataGenerator(\n    preprocessing_function=tf.keras.applications.efficientnet.preprocess_input,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest',\n    validation_split=0.2\n)\n\ntest_datagen = ImageDataGenerator(\n    preprocessing_function=tf.keras.applications.efficientnet.preprocess_input\n)\n\ndef train_model(model, base_model, train_generator, validation_generator, epochs=50):\n    # First phase: Train only the top layers\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.1),\n        metrics=['accuracy']\n    )\n    \n    callbacks = [\n        EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n        ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6),\n        ModelCheckpoint('best_efficientnet_model.keras', monitor='val_loss', save_best_only=True),\n        CSVLogger(os.path.join(log_dir, 'efficientnet_training.csv'))\n    ]\n    \n    # First phase training\n    history_1 = model.fit(\n        train_generator,\n        validation_data=validation_generator,\n        epochs=epochs//2,\n        callbacks=callbacks\n    )\n    \n    # Second phase: Fine-tune the last few layers of the base model\n    # Unfreeze the last 20 layers\n    base_model.trainable = True\n    for layer in base_model.layers[:-20]:\n        layer.trainable = False\n    \n    # Recompile with lower learning rate\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.1),\n        metrics=['accuracy']\n    )\n    \n    # Second phase training\n    history_2 = model.fit(\n        train_generator,\n        validation_data=validation_generator,\n        epochs=epochs//2,\n        callbacks=callbacks\n    )\n    \n    return history_1, history_2\n\n# Data generators setup\ntrain_generator = train_datagen.flow_from_directory(\n    '/kaggle/input/chest-x-ray-images/FinalData/train',\n    target_size=(128, 128),\n    batch_size=32,\n    class_mode='binary',\n    subset='training'\n)\n\nvalidation_generator = train_datagen.flow_from_directory(\n    '/kaggle/input/chest-x-ray-images/FinalData/val',\n    target_size=(128, 128),\n    batch_size=32,\n    class_mode='binary',\n    subset='validation'\n)\n\ntest_generator = test_datagen.flow_from_directory(\n    '/kaggle/input/chest-x-ray-images/FinalData/test',\n    target_size=(128, 128),\n    batch_size=32,\n    class_mode='binary'\n)\ndef create_results_archive(run_dir, timestamp):\n    \"\"\"Create zip archive of all results\"\"\"\n    zip_path = f'/kaggle/working/model_results_{timestamp}.zip'\n    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        for root, dirs, files in os.walk(run_dir):\n            for file in files:\n                file_path = os.path.join(root, file)\n                arcname = os.path.relpath(file_path, run_dir)\n                zipf.write(file_path, arcname)\n    return zip_path\n\n# Main execution\nif __name__ == \"__main__\":\n    logging.info(\"Starting model training pipeline\")\n    \n    try:\n        # Create and train model\n        model, base_model = create_efficient_net_model()\n        logging.info(\"Model created successfully\")\n        \n        start_time = time.time()\n        history_1, history_2 = train_model(model, base_model, train_generator, validation_generator)\n        execution_time = time.time() - start_time\n        \n        # Evaluate model\n        y_true = validation_generator.classes\n        y_pred = (model.predict(validation_generator) > 0.5).astype(\"int32\")\n        \n        # Compute metrics\n        metrics = {\n            'Accuracy': accuracy_score(y_true, y_pred),\n            'Precision': precision_score(y_true, y_pred),\n            'Recall': recall_score(y_true, y_pred),\n            'F1-score': f1_score(y_true, y_pred),\n            'Execution Time': f\"{execution_time:.2f} seconds\"\n        }\n        \n        # Generate plots and save results\n        plot_training_history(history_1, history_2, plot_dir)\n        plot_confusion_matrix(y_true, y_pred, plot_dir)\n        save_metrics_report(metrics, log_dir)\n        \n        # Create zip archive\n        zip_path = create_results_archive(run_dir, timestamp)\n        \n        logging.info(\"Training pipeline completed successfully\")\n        logging.info(f\"Results archived at: {zip_path}\")\n        \n        # Print final metrics\n        for metric, value in metrics.items():\n            print(f\"{metric}: {value}\")\n            \n    except Exception as e:\n        logging.error(f\"Error during execution: {str(e)}\")\n        raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T11:14:50.143791Z","iopub.execute_input":"2024-11-25T11:14:50.144040Z","iopub.status.idle":"2024-11-25T12:09:26.354769Z","shell.execute_reply.started":"2024-11-25T11:14:50.144011Z","shell.execute_reply":"2024-11-25T12:09:26.353806Z"}},"outputs":[{"name":"stdout","text":"Found 13830 images belonging to 2 classes.\nFound 166 images belonging to 2 classes.\nFound 2096 images belonging to 2 classes.\nDownloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n\u001b[1m16705208/16705208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\nEpoch 1/25\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1732533340.486769     117 service.cc:145] XLA service 0x7f31900016a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1732533340.486821     117 service.cc:153]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m  1/433\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:51:06\u001b[0m 32s/step - accuracy: 0.5312 - loss: 0.7080","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1732533359.876210     117 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m243s\u001b[0m 488ms/step - accuracy: 0.7653 - loss: 0.5444 - val_accuracy: 0.8976 - val_loss: 0.3934 - learning_rate: 0.0010\nEpoch 2/25\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 262ms/step - accuracy: 0.8190 - loss: 0.4754 - val_accuracy: 0.9398 - val_loss: 0.3645 - learning_rate: 0.0010\nEpoch 3/25\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 254ms/step - accuracy: 0.8212 - loss: 0.4669 - val_accuracy: 0.8855 - val_loss: 0.4061 - learning_rate: 0.0010\nEpoch 4/25\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 253ms/step - accuracy: 0.8269 - loss: 0.4566 - val_accuracy: 0.8916 - val_loss: 0.3984 - learning_rate: 0.0010\nEpoch 5/25\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 261ms/step - accuracy: 0.8242 - loss: 0.4549 - val_accuracy: 0.8976 - val_loss: 0.3632 - learning_rate: 0.0010\nEpoch 6/25\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 257ms/step - accuracy: 0.8304 - loss: 0.4527 - val_accuracy: 0.9096 - val_loss: 0.3576 - learning_rate: 0.0010\nEpoch 7/25\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 257ms/step - accuracy: 0.8348 - loss: 0.4491 - val_accuracy: 0.9277 - val_loss: 0.3445 - learning_rate: 0.0010\nEpoch 8/25\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 254ms/step - accuracy: 0.8420 - loss: 0.4452 - val_accuracy: 0.8976 - val_loss: 0.3462 - learning_rate: 0.0010\nEpoch 9/25\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 253ms/step - accuracy: 0.8348 - loss: 0.4478 - val_accuracy: 0.9036 - val_loss: 0.3594 - learning_rate: 0.0010\nEpoch 10/25\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 255ms/step - accuracy: 0.8364 - loss: 0.4464 - val_accuracy: 0.9036 - val_loss: 0.3600 - learning_rate: 0.0010\nEpoch 11/25\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 255ms/step - accuracy: 0.8420 - loss: 0.4360 - val_accuracy: 0.9157 - val_loss: 0.3564 - learning_rate: 2.0000e-04\nEpoch 12/25\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 254ms/step - accuracy: 0.8453 - loss: 0.4291 - val_accuracy: 0.8916 - val_loss: 0.3678 - learning_rate: 2.0000e-04\nEpoch 1/25\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 311ms/step - accuracy: 0.8097 - loss: 0.4871 - val_accuracy: 0.9217 - val_loss: 0.3428 - learning_rate: 1.0000e-04\nEpoch 2/25\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 254ms/step - accuracy: 0.8420 - loss: 0.4410 - val_accuracy: 0.8976 - val_loss: 0.3628 - learning_rate: 1.0000e-04\nEpoch 3/25\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 255ms/step - accuracy: 0.8383 - loss: 0.4343 - val_accuracy: 0.9217 - val_loss: 0.3576 - learning_rate: 1.0000e-04\nEpoch 4/25\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 258ms/step - accuracy: 0.8447 - loss: 0.4320 - val_accuracy: 0.9157 - val_loss: 0.3326 - learning_rate: 1.0000e-04\nEpoch 5/25\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 256ms/step - accuracy: 0.8509 - loss: 0.4194 - val_accuracy: 0.9277 - val_loss: 0.3364 - learning_rate: 1.0000e-04\nEpoch 6/25\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 257ms/step - accuracy: 0.8514 - loss: 0.4188 - val_accuracy: 0.9277 - val_loss: 0.3338 - learning_rate: 1.0000e-04\nEpoch 7/25\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 257ms/step - accuracy: 0.8562 - loss: 0.4150 - val_accuracy: 0.9277 - val_loss: 0.3387 - learning_rate: 1.0000e-04\nEpoch 8/25\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 259ms/step - accuracy: 0.8561 - loss: 0.4118 - val_accuracy: 0.9277 - val_loss: 0.3251 - learning_rate: 2.0000e-05\nEpoch 9/25\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 255ms/step - accuracy: 0.8555 - loss: 0.4145 - val_accuracy: 0.9458 - val_loss: 0.3141 - learning_rate: 2.0000e-05\nEpoch 10/25\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 256ms/step - accuracy: 0.8598 - loss: 0.4053 - val_accuracy: 0.9458 - val_loss: 0.3087 - learning_rate: 2.0000e-05\nEpoch 11/25\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 258ms/step - accuracy: 0.8575 - loss: 0.4049 - val_accuracy: 0.9277 - val_loss: 0.3262 - learning_rate: 2.0000e-05\nEpoch 12/25\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 257ms/step - accuracy: 0.8636 - loss: 0.4043 - val_accuracy: 0.9337 - val_loss: 0.3363 - learning_rate: 2.0000e-05\nEpoch 13/25\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 256ms/step - accuracy: 0.8613 - loss: 0.4073 - val_accuracy: 0.9277 - val_loss: 0.3375 - learning_rate: 2.0000e-05\nEpoch 14/25\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 255ms/step - accuracy: 0.8641 - loss: 0.4007 - val_accuracy: 0.9217 - val_loss: 0.3265 - learning_rate: 4.0000e-06\nEpoch 15/25\n\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 255ms/step - accuracy: 0.8648 - loss: 0.4010 - val_accuracy: 0.9337 - val_loss: 0.3314 - learning_rate: 4.0000e-06\n\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step \nAccuracy: 0.5180722891566265\nPrecision: 0.6435643564356436\nRecall: 0.5963302752293578\nF1-score: 0.6190476190476191\nExecution Time: 3227.97 seconds\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import shutil\n\n# Compress the output directory\nshutil.make_archive(\"/kaggle/working/model_logs\", 'zip', \"/kaggle/working/\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T12:45:36.192900Z","iopub.execute_input":"2024-11-25T12:45:36.193335Z","iopub.status.idle":"2024-11-25T12:45:36.223370Z","shell.execute_reply.started":"2024-11-25T12:45:36.193283Z","shell.execute_reply":"2024-11-25T12:45:36.222326Z"}},"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/model_logs.zip'"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"from IPython.display import FileLink \nFileLink(r'model_logs.zip')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T12:42:47.410452Z","iopub.execute_input":"2024-11-25T12:42:47.411165Z","iopub.status.idle":"2024-11-25T12:42:47.417177Z","shell.execute_reply.started":"2024-11-25T12:42:47.411131Z","shell.execute_reply":"2024-11-25T12:42:47.416331Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/model_logs.zip","text/html":"<a href='model_logs.zip' target='_blank'>model_logs.zip</a><br>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}