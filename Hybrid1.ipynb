{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 16:15:50.171073: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-10 16:15:50.561633: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-11-10 16:15:50.561649: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-11-10 16:15:51.607360: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-11-10 16:15:51.607423: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-11-10 16:15:51.607432: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import pandas as pd  \n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = r'/home/prashantb/Documents/Prashant/Thesis/FinalData'\n",
    "\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "test_dir = os.path.join(base_dir, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2  # 20% of training data used as validation data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13964 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Data generator for training data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    '/home/prashantb/Documents/Prashant/Thesis/FinalData/train',\n",
    "    target_size=(128, 128),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    subset='training'  # Set as training data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3490 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Data generator for validation data\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    '/home/prashantb/Documents/Prashant/Thesis/FinalData/train',\n",
    "    target_size=(128, 128),\n",
    "    batch_size=16,\n",
    "    class_mode='binary',\n",
    "    subset='validation'  # Set as validation data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2096 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Data generator for test data (no augmentation)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    '/home/prashantb/Documents/Prashant/Thesis/FinalData/test',\n",
    "    target_size=(128, 128),\n",
    "    batch_size=16,\n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Define the simplified CNN model\n",
    "def create_cnn_model():\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3), kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "\n",
    "        Conv2D(64, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "\n",
    "        Conv2D(128, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "\n",
    "        # Global Average Pooling layer instead of Flattening\n",
    "        GlobalAveragePooling2D(),\n",
    "\n",
    "        # Fully connected dense layer\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.3),  # Reduced Dropout rate\n",
    "\n",
    "        # Output layer with sigmoid for binary classification\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dense, Dropout, BatchNormalization, Input, Concatenate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vision Transformer parameters\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "mlp_dim = 2048\n",
    "hidden_dim = 512\n",
    "patch_size = 16\n",
    "num_patches = (128 // patch_size) ** 2  # (128 / 16)^2 = 64 patches\n",
    "dropout_rate = 0.1\n",
    "\n",
    "# Patch and Position Embedding Layer\n",
    "class PatchEmbedding(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded\n",
    "\n",
    "# Transformer Block\n",
    "def transformer_block(inputs, num_heads, mlp_dim, dropout_rate):\n",
    "    x1 = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    attention_output = layers.MultiHeadAttention(\n",
    "        num_heads=num_heads, key_dim=hidden_dim, dropout=dropout_rate\n",
    "    )(x1, x1)\n",
    "    x2 = layers.Add()([attention_output, inputs])\n",
    "\n",
    "    x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "    x3 = layers.Dense(mlp_dim, activation=tf.nn.gelu)(x3)\n",
    "    x3 = layers.Dropout(dropout_rate)(x3)\n",
    "    x3 = layers.Dense(hidden_dim)(x3)\n",
    "    return layers.Add()([x3, x2])\n",
    "\n",
    "# Vision Transformer Model\n",
    "def create_vit_classifier():\n",
    "    inputs = layers.Input(shape=(128, 128, 3))\n",
    "    patches = layers.Conv2D(hidden_dim, kernel_size=patch_size, strides=patch_size)(inputs)\n",
    "    patches = layers.Reshape((num_patches, hidden_dim))(patches)\n",
    "    encoded_patches = PatchEmbedding(num_patches, hidden_dim)(patches)\n",
    "\n",
    "    for _ in range(num_layers):\n",
    "        encoded_patches = transformer_block(encoded_patches, num_heads, mlp_dim, dropout_rate)\n",
    "\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "    features = layers.Dense(mlp_dim, activation=tf.nn.gelu)(representation)\n",
    "    logits = layers.Dense(1)(features)\n",
    "    outputs = layers.Activation(\"sigmoid\")(logits)\n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 16:19:45.249499: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2024-11-10 16:19:45.249849: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2024-11-10 16:19:45.249890: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (prashantB-viveka): /proc/driver/nvidia/version does not exist\n",
      "2024-11-10 16:19:45.250934: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-10 16:19:46.086796: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 268435456 exceeds 10% of free system memory.\n",
      "2024-11-10 16:19:46.152785: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 268435456 exceeds 10% of free system memory.\n",
      "2024-11-10 16:19:46.195188: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 268435456 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "# Function to remove the last layer and get the feature extraction part of the model\n",
    "def get_feature_extractor(model):\n",
    "    model = Model(inputs=model.input, outputs=model.layers[-2].output)\n",
    "    return model\n",
    "\n",
    "# Define the hybrid model\n",
    "def create_hybrid_model(cnn_model, vit_model, input_shape):\n",
    "    # Create input layer\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    \n",
    "    # Get the CNN and ViT feature extraction parts\n",
    "    cnn_features = cnn_model(input_layer)\n",
    "    vit_features = vit_model(input_layer)\n",
    "    \n",
    "    # Concatenate the features\n",
    "    combined_features = Concatenate()([cnn_features, vit_features])\n",
    "    \n",
    "    # Add a few dense layers for final classification\n",
    "    x = Dense(512, activation='relu')(combined_features)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output_layer = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    # Create the hybrid model\n",
    "    hybrid_model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    return hybrid_model\n",
    "\n",
    "# Create the individual models\n",
    "cnn_model = create_cnn_model()\n",
    "vit_model = create_vit_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the feature extraction models\n",
    "cnn_feature_extractor = get_feature_extractor(cnn_model)\n",
    "vit_feature_extractor = get_feature_extractor(vit_model)\n",
    "\n",
    "# Input shape for the models\n",
    "input_shape = (128, 128, 3)\n",
    "\n",
    "# Create the hybrid model\n",
    "hybrid_model = create_hybrid_model(cnn_feature_extractor, vit_feature_extractor, input_shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 128, 128, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " model_7 (Functional)           (None, 256)          127168      ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " model_8 (Functional)           (None, 1)            130822145   ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 257)          0           ['model_7[0][0]',                \n",
      "                                                                  'model_8[0][0]']                \n",
      "                                                                                                  \n",
      " dense_23 (Dense)               (None, 512)          132096      ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 512)          0           ['dense_23[0][0]']               \n",
      "                                                                                                  \n",
      " dense_24 (Dense)               (None, 256)          131328      ['dropout_12[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (None, 256)          0           ['dense_24[0][0]']               \n",
      "                                                                                                  \n",
      " dense_25 (Dense)               (None, 1)            257         ['dropout_13[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 131,212,994\n",
      "Trainable params: 131,212,546\n",
      "Non-trainable params: 448\n",
      "__________________________________________________________________________________________________\n",
      "219/219 [==============================] - 148s 668ms/step - loss: 0.8252 - accuracy: 0.6825 - precision_2: 0.7223 - recall_2: 0.9030\n",
      "Validation Accuracy: 0.6825\n",
      "Validation Precision: 0.7223\n",
      "Validation Recall: 0.9030\n",
      "219/219 [==============================] - 147s 667ms/step\n",
      "Validation F1-score: 0.7865\n"
     ]
    }
   ],
   "source": [
    "# Compile the hybrid model with additional metrics\n",
    "hybrid_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    ")\n",
    "\n",
    "# Print the hybrid model summary\n",
    "hybrid_model.summary()\n",
    "\n",
    "# Assuming you have your data generators ready\n",
    "# Replace `train_generator` and `validation_generator` with your actual generators\n",
    "batch_size = 32\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255,\n",
    "    validation_split=0.2  # Using 20% of data for validation\n",
    ")\n",
    "\n",
    "\n",
    "# Evaluate the hybrid model\n",
    "loss, accuracy, precision, recall = hybrid_model.evaluate(validation_generator)\n",
    "print(f'Validation Accuracy: {accuracy:.4f}')\n",
    "print(f'Validation Precision: {precision:.4f}')\n",
    "print(f'Validation Recall: {recall:.4f}')\n",
    "\n",
    "# Calculate additional metrics\n",
    "y_true = validation_generator.classes\n",
    "y_pred = hybrid_model.predict(validation_generator)\n",
    "y_pred = (y_pred > 0.5).astype(int).reshape(-1)\n",
    "\n",
    "# Calculate F1-score\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "print(f'Validation F1-score: {f1:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.27      0.11      0.16       996\n",
      "     Class 1       0.71      0.88      0.79      2494\n",
      "\n",
      "    accuracy                           0.66      3490\n",
      "   macro avg       0.49      0.50      0.47      3490\n",
      "weighted avg       0.59      0.66      0.61      3490\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification report\n",
    "report = classification_report(y_true, y_pred, target_names=['Class 0', 'Class 1'])\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
