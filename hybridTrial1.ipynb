{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 23:41:05.643536: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-16 23:41:06.051489: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-11-16 23:41:06.051505: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-11-16 23:41:07.436952: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-11-16 23:41:07.437071: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-11-16 23:41:07.437084: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/prashantb/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import pandas as pd  \n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models  # Ensure this line is included\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, classification_report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = r'/home/prashantb/Documents/Prashant/Thesis/FinalData'\n",
    "\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "test_dir = os.path.join(base_dir, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2  # 20% of training data used as validation data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13964 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Data generator for training data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    '/home/prashantb/Documents/Prashant/Thesis/FinalData/train',\n",
    "    target_size=(128, 128),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    subset='training'  # Set as training data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3490 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Data generator for validation data\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    '/home/prashantb/Documents/Prashant/Thesis/FinalData/train',\n",
    "    target_size=(128, 128),\n",
    "    batch_size=16,\n",
    "    class_mode='binary',\n",
    "    subset='validation'  # Set as validation data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2096 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Data generator for test data (no augmentation)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    '/home/prashantb/Documents/Prashant/Thesis/FinalData/test',\n",
    "    target_size=(128, 128),\n",
    "    batch_size=16,\n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 23:41:39.882048: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2024-11-16 23:41:39.882345: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2024-11-16 23:41:39.882360: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (prashantB-viveka): /proc/driver/nvidia/version does not exist\n",
      "2024-11-16 23:41:39.883297: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 128, 128, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " sequential (Sequential)        (None, 1)            5745729     ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " model (Functional)             (None, 1)            130822145   ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 2)            0           ['sequential[0][0]',             \n",
      "                                                                  'model[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 1024)         3072        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 1024)         0           ['dense_17[0][0]']               \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 512)          524800      ['dropout_12[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (None, 512)          0           ['dense_18[0][0]']               \n",
      "                                                                                                  \n",
      " dense_19 (Dense)               (None, 1)            513         ['dropout_13[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 137,096,259\n",
      "Trainable params: 137,092,419\n",
      "Non-trainable params: 3,840\n",
      "__________________________________________________________________________________________________\n",
      "Found 13964 images belonging to 2 classes.\n",
      "Found 3490 images belonging to 2 classes.\n",
      "Found 2096 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dense, Dropout, BatchNormalization, Input, Concatenate\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# CNN Model Definition\n",
    "cnn_model = Sequential([\n",
    "    Conv2D(64, (3, 3), activation='relu', input_shape=(128, 128, 3), kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(64, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.1),\n",
    "\n",
    "    Conv2D(128, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(128, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.1),\n",
    "\n",
    "    Conv2D(256, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(256, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.1),\n",
    "\n",
    "    Conv2D(512, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(512, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.1),\n",
    "\n",
    "    GlobalAveragePooling2D(),\n",
    "\n",
    "    Dense(2048, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Vision Transformer (Hybrid) Model Definition\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "mlp_dim = 2048\n",
    "hidden_dim = 512\n",
    "patch_size = 16\n",
    "num_patches = (128 // patch_size) ** 2  # (128 / 16)^2 = 64 patches\n",
    "dropout_rate = 0.1\n",
    "\n",
    "# Patch and Position Embedding Layer\n",
    "class PatchEmbedding(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded\n",
    "\n",
    "# Transformer Block\n",
    "def transformer_block(inputs, num_heads, mlp_dim, dropout_rate):\n",
    "    x1 = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    attention_output = layers.MultiHeadAttention(\n",
    "        num_heads=num_heads, key_dim=hidden_dim, dropout=dropout_rate\n",
    "    )(x1, x1)\n",
    "    x2 = layers.Add()([attention_output, inputs])\n",
    "\n",
    "    x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "    x3 = layers.Dense(mlp_dim, activation=tf.nn.gelu)(x3)\n",
    "    x3 = layers.Dropout(dropout_rate)(x3)\n",
    "    x3 = layers.Dense(hidden_dim)(x3)\n",
    "    return layers.Add()([x3, x2])\n",
    "\n",
    "# ViT Model\n",
    "def create_vit_classifier():\n",
    "    inputs = layers.Input(shape=(128, 128, 3))\n",
    "    patches = layers.Conv2D(hidden_dim, kernel_size=patch_size, strides=patch_size)(inputs)\n",
    "    patches = layers.Reshape((num_patches, hidden_dim))(patches)\n",
    "    encoded_patches = PatchEmbedding(num_patches, hidden_dim)(patches)\n",
    "\n",
    "    for _ in range(num_layers):\n",
    "        encoded_patches = transformer_block(encoded_patches, num_heads, mlp_dim, dropout_rate)\n",
    "\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "    features = layers.Dense(mlp_dim, activation=tf.nn.gelu)(representation)\n",
    "    logits = layers.Dense(1)(features)\n",
    "    outputs = layers.Activation(\"sigmoid\")(logits)\n",
    "    return models.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Create ViT model\n",
    "vit_model = create_vit_classifier()\n",
    "\n",
    "# Hybrid Model\n",
    "input_layer = Input(shape=(128, 128, 3))\n",
    "\n",
    "# CNN Model Output\n",
    "cnn_output = cnn_model(input_layer)\n",
    "\n",
    "# ViT Model Output\n",
    "vit_output = vit_model(input_layer)\n",
    "\n",
    "# Combine outputs from CNN and ViT\n",
    "combined_features = Concatenate()([cnn_output, vit_output])\n",
    "\n",
    "# Additional Dense Layers for the final classification\n",
    "x = Dense(1024, activation='relu')(combined_features)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "# Final output layer for binary classification\n",
    "output_layer = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Create and compile the hybrid model\n",
    "hybrid_model = models.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile the hybrid model\n",
    "hybrid_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Print the model summary\n",
    "hybrid_model.summary()\n",
    "\n",
    "# Data Generators for Training, Validation, and Testing\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    '/home/prashantb/Documents/Prashant/Thesis/FinalData/train',\n",
    "    target_size=(128, 128),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    '/home/prashantb/Documents/Prashant/Thesis/FinalData/train',\n",
    "    target_size=(128, 128),\n",
    "    batch_size=16,\n",
    "    class_mode='binary',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    '/home/prashantb/Documents/Prashant/Thesis/FinalData/test',\n",
    "    target_size=(128, 128),\n",
    "    batch_size=16,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "437/437 [==============================] - 3191s 7s/step - loss: 1.3604 - accuracy: 0.7119 - val_loss: 1.2434 - val_accuracy: 0.7146\n",
      "Epoch 2/25\n",
      "437/437 [==============================] - 3115s 7s/step - loss: 0.9151 - accuracy: 0.7375 - val_loss: 0.8477 - val_accuracy: 0.7590\n",
      "Epoch 3/25\n",
      "437/437 [==============================] - 3114s 7s/step - loss: 0.7642 - accuracy: 0.7767 - val_loss: 0.6454 - val_accuracy: 0.8458\n",
      "Epoch 4/25\n",
      "437/437 [==============================] - 3115s 7s/step - loss: 0.6838 - accuracy: 0.7928 - val_loss: 0.5212 - val_accuracy: 0.8894\n",
      "Epoch 5/25\n",
      "437/437 [==============================] - 3117s 7s/step - loss: 0.6402 - accuracy: 0.7965 - val_loss: 0.5239 - val_accuracy: 0.8880\n",
      "Epoch 6/25\n",
      "437/437 [==============================] - 3117s 7s/step - loss: 0.6071 - accuracy: 0.8025 - val_loss: 0.4307 - val_accuracy: 0.8968\n",
      "Epoch 7/25\n",
      "437/437 [==============================] - 3116s 7s/step - loss: 0.5851 - accuracy: 0.8054 - val_loss: 0.4996 - val_accuracy: 0.8662\n",
      "Epoch 8/25\n",
      "437/437 [==============================] - 3115s 7s/step - loss: 0.5565 - accuracy: 0.8145 - val_loss: 0.4766 - val_accuracy: 0.8708\n",
      "Epoch 9/25\n",
      "437/437 [==============================] - 3117s 7s/step - loss: 0.5415 - accuracy: 0.8164 - val_loss: 0.5206 - val_accuracy: 0.8255\n",
      "Epoch 10/25\n",
      "437/437 [==============================] - 3115s 7s/step - loss: 0.5277 - accuracy: 0.8170 - val_loss: 0.3979 - val_accuracy: 0.8633\n",
      "Epoch 11/25\n",
      "437/437 [==============================] - 3116s 7s/step - loss: 0.5088 - accuracy: 0.8243 - val_loss: 0.3868 - val_accuracy: 0.9023\n",
      "Epoch 12/25\n",
      "437/437 [==============================] - 3143s 7s/step - loss: 0.4972 - accuracy: 0.8279 - val_loss: 0.3174 - val_accuracy: 0.9307\n",
      "Epoch 13/25\n",
      "437/437 [==============================] - 3201s 7s/step - loss: 0.4920 - accuracy: 0.8263 - val_loss: 0.2953 - val_accuracy: 0.9347\n",
      "Epoch 14/25\n",
      "437/437 [==============================] - 3102s 7s/step - loss: 0.4856 - accuracy: 0.8250 - val_loss: 0.7166 - val_accuracy: 0.7255\n",
      "Epoch 15/25\n",
      "437/437 [==============================] - 3050s 7s/step - loss: 0.4750 - accuracy: 0.8319 - val_loss: 0.3337 - val_accuracy: 0.9066\n",
      "Epoch 16/25\n",
      "437/437 [==============================] - 3137s 7s/step - loss: 0.4633 - accuracy: 0.8328 - val_loss: 0.3633 - val_accuracy: 0.8946\n",
      "Epoch 17/25\n",
      "437/437 [==============================] - 3205s 7s/step - loss: 0.4641 - accuracy: 0.8300 - val_loss: 0.5041 - val_accuracy: 0.7748\n",
      "Epoch 18/25\n",
      "437/437 [==============================] - 3219s 7s/step - loss: 0.4644 - accuracy: 0.8284 - val_loss: 0.3562 - val_accuracy: 0.8920\n",
      "Epoch 19/25\n",
      "437/437 [==============================] - 3226s 7s/step - loss: 0.4544 - accuracy: 0.8341 - val_loss: 0.7626 - val_accuracy: 0.7487\n",
      "Epoch 20/25\n",
      "437/437 [==============================] - 3262s 7s/step - loss: 0.4484 - accuracy: 0.8365 - val_loss: 0.3657 - val_accuracy: 0.8682\n",
      "Epoch 21/25\n",
      "437/437 [==============================] - 3211s 7s/step - loss: 0.4406 - accuracy: 0.8387 - val_loss: 0.5024 - val_accuracy: 0.7934\n",
      "Epoch 22/25\n",
      "437/437 [==============================] - 3228s 7s/step - loss: 0.4415 - accuracy: 0.8367 - val_loss: 0.9763 - val_accuracy: 0.6287\n",
      "Epoch 23/25\n",
      "437/437 [==============================] - 3247s 7s/step - loss: 0.4397 - accuracy: 0.8361 - val_loss: 0.2474 - val_accuracy: 0.9430\n",
      "Epoch 24/25\n",
      "437/437 [==============================] - 3189s 7s/step - loss: 0.4345 - accuracy: 0.8407 - val_loss: 0.2324 - val_accuracy: 0.9499\n",
      "Epoch 25/25\n",
      "437/437 [==============================] - 3197s 7s/step - loss: 0.4339 - accuracy: 0.8386 - val_loss: 0.2705 - val_accuracy: 0.9309\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss')\n",
    "\n",
    "# Train the hybrid model\n",
    "history = hybrid_model.fit(\n",
    "    train_generator,\n",
    "    epochs=25,  # Adjust epochs as needed\n",
    "    validation_data=validation_generator\n",
    ")\n",
    "\n",
    "# Log the performance\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(filename='hybrid_model_performance.log', level=logging.INFO)\n",
    "\n",
    "logging.info(f\"Training Accuracy: {history.history['accuracy']}\")\n",
    "logging.info(f\"Validation Accuracy: {history.history['val_accuracy']}\")\n",
    "logging.info(f\"Training Loss: {history.history['loss']}\")\n",
    "logging.info(f\"Validation Loss: {history.history['val_loss']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/131 [==============================] - 131s 958ms/step\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.33      0.33       701\n",
      "           1       0.66      0.66      0.66      1395\n",
      "\n",
      "    accuracy                           0.55      2096\n",
      "   macro avg       0.49      0.49      0.49      2096\n",
      "weighted avg       0.55      0.55      0.55      2096\n",
      "\n",
      "Confusion Matrix:\n",
      "[[229 472]\n",
      " [479 916]]\n",
      "219/219 [==============================] - 230s 1s/step\n",
      "Hybrid Model - Accuracy: 0.5971\n",
      "Hybrid Model - Precision: 0.7130\n",
      "Hybrid Model - Recall: 0.7302\n",
      "Hybrid Model - F1-score: 0.7215\n",
      "219/219 [==============================] - 173s 783ms/step\n",
      "ViT Model - Accuracy: 0.2854\n",
      "ViT Model - Precision: 0.0000\n",
      "ViT Model - Recall: 0.0000\n",
      "ViT Model - F1-score: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prashantb/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_true = test_generator.classes\n",
    "y_pred = hybrid_model.predict(test_generator)\n",
    "y_pred = (y_pred > 0.5).astype(int)  # Convert probability to binary class\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "# Evaluate the model on the validation data\n",
    "y_true = validation_generator.classes\n",
    "y_pred = (hybrid_model.predict(validation_generator) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "# Log metrics\n",
    "logging.info(f'Hybrid Model - Accuracy: {accuracy:.4f}')\n",
    "logging.info(f'Hybrid Model - Precision: {precision:.4f}')\n",
    "logging.info(f'Hybrid Model - Recall: {recall:.4f}')\n",
    "logging.info(f'Hybrid Model - F1-score: {f1:.4f}')\n",
    "\n",
    "# Output results to the console\n",
    "print(f\"Hybrid Model - Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Hybrid Model - Precision: {precision:.4f}\")\n",
    "print(f\"Hybrid Model - Recall: {recall:.4f}\")\n",
    "print(f\"Hybrid Model - F1-score: {f1:.4f}\")\n",
    "\n",
    "\n",
    "y_pred = (vit_model.predict(validation_generator) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "# Log metrics\n",
    "logging.info(f'ViT Model - Accuracy: {accuracy:.4f}')\n",
    "logging.info(f'ViT Model - Precision: {precision:.4f}')\n",
    "logging.info(f'ViT Model - Recall: {recall:.4f}')\n",
    "logging.info(f'ViT Model - F1-score: {f1:.4f}')\n",
    "\n",
    "# Output results to the console\n",
    "print(f\"ViT Model - Accuracy: {accuracy:.4f}\")\n",
    "print(f\"ViT Model - Precision: {precision:.4f}\")\n",
    "print(f\"ViT Model - Recall: {recall:.4f}\")\n",
    "print(f\"ViT Model - F1-score: {f1:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
